{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrCvBU+sQZXIOoibzGy9JA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aakash326/ML-models/blob/main/tutor_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ir-T_J1AKQeV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import display, Markdown\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"\")\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "MPla6YkQKROu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown in Jupyter.\"\"\"\n",
        "    display(Markdown(text))"
      ],
      "metadata": {
        "id": "VR6qFoipKYol"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ai_tutor_response(user_question):\n",
        "    \"\"\"\n",
        "    Sends a question to the Gemini API, asking it to respond as an AI Tutor.\n",
        "\n",
        "    Args:\n",
        "        user_question (str): The question asked by the user.\n",
        "\n",
        "    Returns:\n",
        "        str: The AI's response, or an error message.\n",
        "    \"\"\"\n",
        "    # Define the system prompt - instructions for the AI's personality and role\n",
        "    system_prompt = \"\"\"You are a helpful and patient AI Tutor. Explain concepts clearly and step-by-step.\n",
        "    Use examples where helpful. Be encouraging and supportive in your responses.\n",
        "    Always aim to help the student understand the concept fully.\"\"\"\n",
        "\n",
        "    # Combine system prompt with user question\n",
        "    full_prompt = f\"system message: {system_prompt}\\n\\nStudent Question: {user_question}\"\n",
        "        # Make the API call to Gemini\n",
        "    response = model.generate_content(\n",
        "        full_prompt,\n",
        "        generation_config=genai.types.GenerationConfig(\n",
        "              temperature=0.7,\n",
        "              # max_output_tokens=500,  # Limit response length\n",
        "              # top_p=0.8,             # Focus responses\n",
        "              # top_k=10               # Reduce token choices\n",
        "            )# Updated parameter name (replaces allow_flagging)\n",
        "    )\n",
        "    # Extract the answer content\n",
        "    ai_response = response.text\n",
        "    print_markdown(ai_response)\n",
        "\n"
      ],
      "metadata": {
        "id": "I7D3ijT7K2AW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ai_tutor_response_streaming(user_question):\n",
        "    \"\"\"\n",
        "    Sends a question to the Gemini API with streaming response.\n",
        "    Args:\n",
        "        user_question (str): The question asked by the user.\n",
        "    Yields:\n",
        "        str: Partial AI responses as they come in.\n",
        "    \"\"\"\n",
        "    # Define the system prompt\n",
        "    system_prompt = \"\"\"You are a helpful and patient AI Tutor. Explain concepts clearly and step-by-step.\n",
        "    Use examples where helpful. Be encouraging and supportive in your responses.\n",
        "    Always aim to help the student understand the concept fully.\"\"\"\n",
        "\n",
        "    # Combine system prompt with user question\n",
        "    full_prompt = f\"system message: {system_prompt}\\n\\nStudent Question: {user_question}\"\n",
        "\n",
        "    try:\n",
        "        # Make the streaming API call to Gemini\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0.7,\n",
        "                # max_output_tokens=500,\n",
        "                # top_p=0.8,\n",
        "                # top_k=10\n",
        "            ),\n",
        "            stream=True  # Enable streaming\n",
        "        )\n",
        "\n",
        "        # Accumulate the response\n",
        "        full_response = \"\"\n",
        "        for chunk in response:\n",
        "            if chunk.text:\n",
        "                full_response += chunk.text\n",
        "                yield full_response  # Yield the accumulated response\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "3InOYLRqZguq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promp=\"Explain me about RAG\"\n",
        "get_ai_tutor_response(promp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "VoqnNoL5LpVs",
        "outputId": "d4ea6a11-2c56-443a-ee30-591415d122a2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi there!  Let's explore RAG together.  It might seem a bit intimidating at first, but I promise it's much simpler than it sounds. RAG stands for **Retrieval Augmented Generation**.  It's a powerful technique used in artificial intelligence, particularly in large language models (LLMs) like me!\n\nThink of it like this:  Imagine you're writing an essay, and you need to cite your sources.  You wouldn't just make things up, right? You'd look up information in books, articles, and websites, and then use that information to support your points.  RAG does something very similar for LLMs.\n\nHere's a breakdown of how it works, step-by-step:\n\n1. **Retrieval:**  This is the \"Retrieval\" part of RAG.  When you ask me a question, instead of just relying on my existing knowledge (which is vast, but not all-encompassing), I first search a large database of information. This database could be anything from a collection of documents, websites, or even your own personal files.  I use sophisticated techniques to find the most relevant pieces of information related to your question.  This is like you searching Google or a library catalog.\n\n2. **Augmentation:**  This is the \"Augmentation\" part.  Once I've found the relevant information, I \"augment\" my own capabilities by using that information to answer your question.  I don't just regurgitate the information; I use it to create a more accurate, detailed, and contextually appropriate response.  Think of it as taking notes from your research and then writing a well-structured essay based on those notes.\n\n3. **Generation:**  This is the \"Generation\" part.  Finally, I use my language model abilities to generate a human-readable response.  This response incorporates the information I retrieved, ensuring that my answer is grounded in factual information and not just based on my pre-existing knowledge.  This is like writing the final draft of your essay, incorporating all your research.\n\n**Example:**\n\nLet's say you ask me:  \"What was the population of Paris in 1850?\"\n\nWithout RAG: I might give you a general estimate based on my training data, but it might not be very precise.\n\nWith RAG: I would first search a database containing historical census data or historical records about Paris.  I would then retrieve the relevant population data for 1850.  Finally, I would generate a response like: \"According to the [Source Name] census, the population of Paris in 1850 was approximately [Specific Number].\"  This response is more accurate and reliable because it's backed by specific data.\n\n**Why is RAG important?**\n\n* **Accuracy:**  It improves the accuracy of LLM responses by grounding them in factual information.\n* **Up-to-dateness:** LLMs are trained on a fixed dataset. RAG allows them to access and utilize more recent information.\n* **Contextual Understanding:**  It allows LLMs to better understand the context of a question by considering relevant external information.\n\n\nDon't hesitate to ask if anything is unclear! We can go through more examples or delve deeper into any specific aspect of RAG that you'd like to understand better.  You're doing great!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "question = \"Explain machine learning\"\n",
        "for response_chunk in get_ai_tutor_response_streaming(question):\n",
        "    clear_output(wait=True)\n",
        "    print_markdown(response_chunk)\n",
        "    time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "VWdsJsX9ap0i",
        "outputId": "767226d8-7f1b-4407-b4cd-70b40b80cbf7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I can definitely help you understand machine learning! It might sound intimidating, but at its core, it's a pretty intuitive concept.\n\n**Think of it this way:** Imagine you're teaching a dog a new trick. You don't give the dog a set of explicit instructions like a computer program. Instead, you show the dog what you want, reward the desired behavior, and correct the undesired behavior. Over time, the dog *learns* to perform the trick.\n\nMachine learning is similar! Instead of explicitly programming a computer to do something, we **give it data and let it learn patterns and make predictions from that data.**\n\nHere's a more structured breakdown:\n\n**1. What is Machine Learning?**\n\n*   Machine learning (ML) is a field of computer science that focuses on enabling computers to learn from data without being explicitly programmed.\n*   Instead of writing code that tells the computer exactly what to do in every situation, we provide the computer with data, and it learns to identify patterns, make predictions, and improve its performance over time.\n\n**2. Key Concepts:**\n\n*   **Data:** The foundation of machine learning. This can be anything from images and text to numbers and sensor readings.\n*   **Algorithms:** These are the \"recipes\" that the computer uses to learn from the data. There are many different types of algorithms, each suited for different tasks.\n*   **Training:** The process of feeding data to the algorithm so it can learn the patterns and relationships within the data.\n*   **Model:** The result of the training process. It's a representation of what the algorithm has learned, which can then be used to make predictions on new, unseen data.\n*   **Prediction:** The output of the model when it's given new data. It's the model's attempt to guess what the correct answer should be based on what it has learned.\n\n**3. The Machine Learning Process (Simplified):**\n\n1.  **Collect Data:** Gather a large dataset relevant to the problem you're trying to solve. For example, if you want to build a system that recognizes cats in pictures, you'll need a dataset of cat images.\n2.  **Prepare Data:** Clean and format the data so it's suitable for the algorithm. This might involve removing errors, filling in missing values, and transforming the data into a numerical format.\n3.  **Choose a Model:** Select the appropriate machine learning algorithm for the task. There are many different types of models, such as:\n    *   **Linear Regression:** For predicting continuous values (e.g., house prices).\n    *   **Logistic Regression:** For predicting categories (e.g., spam or not spam).\n    *   **Decision Trees:** For making decisions based on a set of rules.\n    *   **Neural Networks:** For complex tasks like image recognition and natural language processing.\n4.  **Train the Model:** Feed the prepared data to the chosen algorithm. The algorithm will analyze the data and adjust its internal parameters to learn the patterns and relationships within the data.\n5.  **Evaluate the Model:** Test the model'"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-3048500501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_tutor_interface_simple = gr.Interface(\n",
        "    fn=get_ai_tutor_response,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask the AI Tutor anything...\"),\n",
        "    outputs=gr.Textbox(label=\"AI Tutor's Answer\"),\n",
        "    title=\"🤖 Simple AI Tutor (Gemini)\",\n",
        "    description=\"Enter your question below and the AI Tutor will provide a helpful explanation!\",\n",
        "    flagging_mode=\"never\",\n",
        ")\n",
        "    # Add these to your generation_config:"
      ],
      "metadata": {
        "id": "dq8ew00wL0Cf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_tutor_interface_simple.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "_403g_IuX6q4",
        "outputId": "4f2f4be7-426f-4831-93aa-07b006c6eebe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b5d62158ca46f19d08.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b5d62158ca46f19d08.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_ai_tutor_response(user_question):\n",
        "    \"\"\"\n",
        "    Sends a question to the Gemini API and streams the response as a generator.\n",
        "    Args:\n",
        "        user_question (str): The question asked by the user.\n",
        "    Yields:\n",
        "        str: Chunks of the AI's response.\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are a helpful and patient AI Tutor. Explain concepts clearly and concisely.\"\n",
        "\n",
        "    try:\n",
        "        # Configure the model\n",
        "        model = genai.GenerativeModel(\n",
        "            model_name='gemini-2.0-flash',\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0.7,\n",
        "            ),\n",
        "            system_instruction=system_prompt\n",
        "        )\n",
        "\n",
        "        # Generate streaming response\n",
        "        response = model.generate_content(\n",
        "            user_question,\n",
        "            stream=True  # Enable streaming\n",
        "        )\n",
        "\n",
        "        full_response = \"\"  # Keep track of the full response\n",
        "\n",
        "        # Loop through each chunk of the response as it arrives\n",
        "        for chunk in response:\n",
        "            if chunk.text:\n",
        "                # Add this chunk to our growing response\n",
        "                full_response += chunk.text\n",
        "                # Yield the accumulated response for real-time display\n",
        "                yield full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during streaming: {e}\")\n",
        "        yield f\"Sorry, I encountered an error: {e}\""
      ],
      "metadata": {
        "id": "1a0vhYquX_y7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream_ai_tutor_response(\"Explain about NLP\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tynqXg6vbajC",
        "outputId": "1454ebbe-b0d0-4b24-b406-07400831d889"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object stream_ai_tutor_response at 0x781454d186a0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_streaming_response(user_question):\n",
        "    \"\"\"Display streaming response with typewriter effect in Colab\"\"\"\n",
        "    print(\"Streaming response:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for response_chunk in stream_ai_tutor_response(user_question):\n",
        "        clear_output(wait=True)\n",
        "        display(Markdown(response_chunk))\n",
        "        time.sleep(0.4)  # Small delay to see the streaming effect"
      ],
      "metadata": {
        "id": "aJpfSyoEbuw5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_streaming_response(\"Explain about NLP\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "O-mV2jhUcbi0",
        "outputId": "e1cd59ec-400b-44cf-d85a-0ee9d8d7dc32"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break down NLP!\n\n**NLP stands for Natural Language Processing.**\n\nIn simple terms, it's about teaching computers how to understand, interpret, and generate human language (like English, Spanish, etc.) in a valuable way.\n\nThink about it: computers are great with structured data (numbers, code), but human language is messy, full of nuances, and context-dependent. NLP bridges that gap.\n\n**Here's a breakdown of what that means:**\n\n*   **Natural Language:** This refers to the languages that humans use to communicate with each other every day.\n*   **Processing:** This involves enabling computers to understand, interpret, and generate human language.\n\n**Here's what NLP allows computers to do:**\n\n*   **Understand Language:** This includes tasks like figuring out the meaning of words, sentences, and even entire documents.\n*   **Generate Language:** This means creating new text that makes sense and is relevant to a specific context.\n*   **Translate Languages:** Converting text from one language to another.\n*   **Analyze Sentiment:** Determining the emotional tone (positive, negative, neutral) of a piece of text.\n*   **Extract Information:** Identifying and pulling out key pieces of data from text.\n*   **Answer Questions:** Providing answers based on information found in text.\n\n**Example:**\n\nImagine you ask your phone's virtual assistant \"What's the weather like today?\". NLP is what allows the phone to:\n\n1.  **Understand** that you're asking about the weather.\n2.  **Process** your request and find the relevant weather information.\n3.  **Generate** a response like \"The weather today is sunny with a high of 75 degrees.\"\n\n**Where is NLP used?**\n\nNLP is used in a wide variety of applications, including:\n\n*   **Chatbots:** To understand and respond to customer inquiries.\n*   **Search Engines:** To provide more relevant search results.\n*   **Machine Translation:** To translate text from one language to another.\n*   **Social Media Monitoring:** To track brand sentiment and identify trends.\n*   **Spam Detection:** To filter out unwanted emails.\n*   **Virtual Assistants:** such as Siri, Alexa, and Google Assistant.\n\n**Key Concepts in NLP:**\n\n*   **Tokenization:** Breaking down text into individual words or units (tokens).\n*   **Part-of-Speech Tagging:** Identifying the grammatical role of each word (noun, verb, adjective, etc.).\n*   **Named Entity Recognition (NER):** Identifying and classifying named entities in text (people, organizations, locations, etc.).\n\n**In summary:** NLP is a field of computer science that focuses on enabling computers to understand, process, and generate human language. It's a rapidly growing field with many applications in a variety of industries.\n\nDo you want to dive deeper into any of these aspects of NLP? For example, I could explain some of the techniques used in NLP, or give more real-world examples. Just let me know!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detailed_explanation_levels = {\n",
        "    1: {\n",
        "        \"description\": \"like I'm 5 years old\",\n",
        "        \"style\": \"Use very simple words, short sentences, and fun examples. Avoid technical terms.\",\n",
        "        \"example_words\": \"big, small, happy, easy, fun\"\n",
        "    },\n",
        "    2: {\n",
        "        \"description\": \"like I'm 10 years old\",\n",
        "        \"style\": \"Use simple language with some basic concepts. Include relatable examples from daily life.\",\n",
        "        \"example_words\": \"understand, explain, because, important\"\n",
        "    },\n",
        "    3: {\n",
        "        \"description\": \"like a high school student\",\n",
        "        \"style\": \"Use moderate vocabulary with some technical terms explained. Include relevant examples.\",\n",
        "        \"example_words\": \"analyze, process, significant, demonstrate\"\n",
        "    },\n",
        "    4: {\n",
        "        \"description\": \"like a college student\",\n",
        "        \"style\": \"Use advanced vocabulary and technical terms. Provide detailed explanations and examples.\",\n",
        "        \"example_words\": \"analyze, synthesize, methodology, implications\"\n",
        "    },\n",
        "    5: {\n",
        "        \"description\": \"like an expert in the field\",\n",
        "        \"style\": \"Use professional terminology and assume deep background knowledge. Focus on nuanced details.\",\n",
        "        \"example_words\": \"paradigm, methodology, implications, optimization\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "3OlDQ2iDcfkt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_ai_tutor_response_with_level_gemini(user_question, explanation_level_value):\n",
        "    \"\"\"\n",
        "    Streams AI Tutor response based on user question and selected explanation level using Gemini.\n",
        "\n",
        "    Args:\n",
        "        user_question (str): The question from the user.\n",
        "        explanation_level_value (int): The value from the slider (1-5).\n",
        "\n",
        "    Yields:\n",
        "        str: Chunks of the AI's response.\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if not user_question.strip():\n",
        "        yield \"Please ask a question!\"\n",
        "        return\n",
        "\n",
        "    # Get the descriptive text for the chosen level\n",
        "    level_info = detailed_explanation_levels.get(\n",
        "        explanation_level_value, \"Clear and conisely\"\n",
        "    )\n",
        "\n",
        "    level_description = level_info[\"description\"]\n",
        "    style_guide = level_info[\"style\"]\n",
        "\n",
        "    # Construct the system prompt dynamically based on the level\n",
        "    system_prompt = f\"\"\"You are a helpful and patient AI Tutor. Your task is to explain concepts {level_description}.\n",
        "\n",
        "    IMPORTANT GUIDELINES for Level {explanation_level_value}:\n",
        "    - {style_guide}\n",
        "    - Adjust your vocabulary, sentence complexity, and examples accordingly\n",
        "    - Make sure your explanation matches the intellectual level requested\n",
        "    - Be encouraging and supportive in your explanations\n",
        "    - Use appropriate formatting (markdown) to make your response clear and readable\n",
        "\n",
        "    Remember: The user is asking for an explanation suitable for someone {level_description}.\"\"\"\n",
        "\n",
        "    # Combine system prompt with user question\n",
        "    full_prompt = f\"{system_prompt}\\n\\nUser Question: {user_question}\"\n",
        "\n",
        "    # Debug print to show the system prompt being used\n",
        "    print(f\"DEBUG: Using explanation level {explanation_level_value} ({level_description})\")\n",
        "    print(f\"DEBUG: Style guide: {style_guide}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the Gemini model\n",
        "        model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "        # Generate response with streaming\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0.7,\n",
        "                max_output_tokens=2000,\n",
        "                top_p=0.8,\n",
        "                top_k=40,\n",
        "            ),\n",
        "            stream=True\n",
        "        )\n",
        "        return display(Markdown(response.text))\n",
        "        # # Stream the response\n",
        "          # full_response = \"\"\n",
        "          # for chunk in response:\n",
        "          # if chunk.text:\n",
        "          # full_response += chunk.text\n",
        "          # yield full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ **Error occurred:** {str(e)}\\n\\nPlease try again or rephrase your question.\"\n",
        "        print(f\"An error occurred during streaming: {e}\")\n",
        "        yield error_msg\n"
      ],
      "metadata": {
        "id": "nhjKzbQcfiBx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ai_tutor_response_with_level_gemini(user_question, explanation_level_value):\n",
        "    \"\"\"\n",
        "    Gets AI Tutor response based on user question and selected explanation level using Gemini.\n",
        "    Args:\n",
        "        user_question (str): The question from the user.\n",
        "        explanation_level_value (int): The value from the slider (1-5).\n",
        "    Returns:\n",
        "        str: The AI's complete response or error message.\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if not user_question.strip():\n",
        "        return \"Please ask a question!\"\n",
        "\n",
        "    # Get the descriptive text for the chosen level\n",
        "    level_info = detailed_explanation_levels.get(\n",
        "        explanation_level_value, {\"description\": \"Clear and concisely\", \"style\": \"Use simple language\"}\n",
        "    )\n",
        "    level_description = level_info[\"description\"]\n",
        "    style_guide = level_info[\"style\"]\n",
        "\n",
        "    # Construct the system prompt dynamically based on the level\n",
        "    system_prompt = f\"\"\"You are a helpful and patient AI Tutor. Your task is to explain concepts {level_description}.\n",
        "\n",
        "    IMPORTANT GUIDELINES for Level {explanation_level_value}:\n",
        "    - {style_guide}\n",
        "    - Adjust your vocabulary, sentence complexity, and examples accordingly\n",
        "    - Make sure your explanation matches the intellectual level requested\n",
        "    - Be encouraging and supportive in your explanations\n",
        "    - Use appropriate formatting (markdown) to make your response clear and readable\n",
        "\n",
        "    Remember: The user is asking for an explanation suitable for someone {level_description}.\"\"\"\n",
        "\n",
        "    # Combine system prompt with user question\n",
        "    full_prompt = f\"{system_prompt}\\n\\nUser Question: {user_question}\"\n",
        "\n",
        "    # Debug print to show the system prompt being used\n",
        "    print(f\"DEBUG: Using explanation level {explanation_level_value} ({level_description})\")\n",
        "    print(f\"DEBUG: Style guide: {style_guide}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the Gemini model\n",
        "        model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "        # Generate response without streaming\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0.7,\n",
        "                max_output_tokens=2000,\n",
        "                top_p=0.8,\n",
        "                top_k=40,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Return the complete response text\n",
        "        print_markdown(response.text)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ **Error occurred:** {str(e)}\\n\\nPlease try again or rephrase your question.\"\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return error_msg"
      ],
      "metadata": {
        "id": "9HKRlc8sgd5a"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this instead of the old function\n",
        "response = get_ai_tutor_response_with_level_gemini(\"What is photosynthesis?\", 3)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "HnYurpEVf9kI",
        "outputId": "434c039a-9464-40e6-c5da-bfdca806d1a4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Using explanation level 3 (like a high school student)\n",
            "DEBUG: Style guide: Use moderate vocabulary with some technical terms explained. Include relevant examples.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hey there! Photosynthesis is a really important process, especially for plants and, well, basically all life on Earth. Let's break it down!\n\n### What it is:\n\nPhotosynthesis is how plants and some other organisms (like algae and certain bacteria) convert light energy into chemical energy. Think of it like plants cooking their own food using sunlight!\n\n**In simple terms:**\n\n*   Plants take in sunlight, water, and carbon dioxide.\n*   They use these ingredients to make glucose (a type of sugar, which is their food) and oxygen.\n*   They release the oxygen into the air, which is what we breathe!\n\n### The Formula:\n\nYou can sum it up with a simple formula:\n\n6CO₂ + 6H₂O + Light Energy → C₆H₁₂O₆ + 6O₂\n\n*   **6CO₂:** Six molecules of carbon dioxide (what we breathe out)\n*   **6H₂O:** Six molecules of water\n*   **Light Energy:** Energy from the sun\n*   **→:** \"Produces\" or \"Converts to\"\n*   **C₆H₁₂O₆:** One molecule of glucose (sugar)\n*   **6O₂:** Six molecules of oxygen (what we breathe in)\n\n### Where it Happens:\n\nPhotosynthesis happens inside plant cells in special compartments called **chloroplasts**. Chloroplasts contain a green pigment called **chlorophyll**, which captures the light energy from the sun. That's why plants are green!\n\n### Why it Matters:\n\n1.  **Food for Plants:** The glucose produced during photosynthesis is the plant's food. It uses this sugar for energy to grow, develop, and do all the things plants need to do.\n2.  **Oxygen for Us:** Photosynthesis releases oxygen into the atmosphere. We, and almost all other animals, need this oxygen to breathe and survive.\n3.  **Foundation of the Food Chain:** Plants are at the bottom of the food chain. They produce their own food through photosynthesis, and then animals eat the plants. Without photosynthesis, there would be no plants, and without plants, the whole food chain would collapse.\n4.  **Balances Carbon Dioxide:** Photosynthesis helps to remove carbon dioxide from the atmosphere, which is a greenhouse gas. By using carbon dioxide, photosynthesis helps regulate the Earth's climate.\n\n### Think of it Like This:\n\nImagine you're baking a cake.\n\n*   **Sunlight** is like your oven, providing the energy to bake.\n*   **Water and Carbon Dioxide** are like the ingredients (flour, eggs, sugar).\n*   **Glucose** is the cake you bake, the final product that provides energy.\n*   **Oxygen** is like the steam that's released as the cake bakes.\n\nSo, photosynthesis is a super important process that not only feeds plants but also provides us with the oxygen we need to breathe and helps maintain the balance of life on Earth. Pretty cool, right?\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ai_tutor_response_with_level_gemini(user_question, explanation_level_value):\n",
        "    \"\"\"\n",
        "    Streams AI Tutor response based on user question and explanation level using Gemini 2.0 Flash.\n",
        "    Displays the result as it generates.\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if not user_question.strip():\n",
        "        return \"Please ask a question!\"\n",
        "\n",
        "    # Level settings\n",
        "    level_info = detailed_explanation_levels.get(\n",
        "        explanation_level_value, {\"description\": \"clearly and concisely\", \"style\": \"Use simple language\"}\n",
        "    )\n",
        "    level_description = level_info[\"description\"]\n",
        "    style_guide = level_info[\"style\"]\n",
        "\n",
        "    # Dynamic system prompt\n",
        "    system_prompt = f\"\"\"You are a helpful and patient AI Tutor. Your task is to explain concepts {level_description}.\n",
        "\n",
        "    GUIDELINES for Level {explanation_level_value}:\n",
        "    - {style_guide}\n",
        "    - Adjust vocabulary, sentence complexity, and examples accordingly\n",
        "    - Use markdown formatting for clarity\n",
        "    \"\"\"\n",
        "\n",
        "    full_prompt = f\"{system_prompt}\\n\\nUser Question: {user_question}\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\n",
        "            model_name='gemini-2.0-flash',\n",
        "            system_instruction=system_prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0.7,\n",
        "                max_output_tokens=2000,\n",
        "                top_p=0.8,\n",
        "                top_k=40,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            stream=True\n",
        "        )\n",
        "\n",
        "        full_response = \"\"\n",
        "        for chunk in response:\n",
        "            if chunk.text:\n",
        "                full_response += chunk.text\n",
        "                clear_output(wait=True)\n",
        "                display(Markdown(full_response))\n",
        "                time.sleep(0.2)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ **Error occurred:** {str(e)}\\n\\nPlease try again or rephrase your question.\"\n",
        "        display(Markdown(error_msg))\n"
      ],
      "metadata": {
        "id": "6nE6zGyTgDTs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_tutor_gradio = gr.Interface(\n",
        "    fn=get_ai_tutor_response_with_level_gemini,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, label=\"Ask the AI Tutor anything...\"),\n",
        "        gr.Slider(1, 5, value=3, step=1, label=\"Explanation Level (1–5)\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"AI Tutor's Answer\", lines=20),\n",
        "    title=\"📘 AI Tutor (Gemini 2.0 Flash)\",\n",
        "    description=\"Ask questions and get streaming explanations at different levels.\",\n",
        "    allow_flagging=\"never\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfZwFICxi_mZ",
        "outputId": "c029cce9-59bb-453b-de06-fec0000bb7ce"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_tutor_gradio.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "AUW1Qaobjmzg",
        "outputId": "3f6549a7-24ba-4ae7-ad9c-4da83024ef6d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0f2163e9b58b73929d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-90-3739633176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mai_tutor_gradio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2905\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnetworking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2906\u001b[0m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                     artifact = HTML(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/networking.py\u001b[0m in \u001b[0;36murl_ok\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     71\u001b[0m             ):  # 401 or 302 if auth is set; 303 or 307 are alternatives to 302 for temporary redirects\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# ✅ Load the secret from Colab\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")  # must match exactly what you named it in the Secrets panel\n",
        "\n",
        "# ✅ Initialize the OpenAI client\n",
        "client = OpenAI(api_key=openai_api_key)\n"
      ],
      "metadata": {
        "id": "d3LZLJfbju6h"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_ai_tutor_response(user_question):\n",
        "    system_prompt = \"You are a helpful and patient AI Tutor. Explain concepts clearly and step-by-step using markdown formatting.\"\n",
        "\n",
        "    try:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_question}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            stream=True,\n",
        "        )\n",
        "\n",
        "        full_response = \"\"\n",
        "        for chunk in stream:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                text_chunk = chunk.choices[0].delta.content\n",
        "                full_response += text_chunk\n",
        "                clear_output(wait=True)\n",
        "                display(Markdown(full_response))\n",
        "                time.sleep(0.1)\n",
        "\n",
        "    except Exception as e:\n",
        "        display(Markdown(f\"❌ **Error:** {str(e)}\"))"
      ],
      "metadata": {
        "id": "VvttONE4ogb7"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Example usage:\n",
        "stream_ai_tutor_response(\"Explain black holes like I'm 10 years old\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "XEzZqAFuo9PK",
        "outputId": "e8b67fe3-8823-4a40-8aa7-defd40c097b5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure! Let’s break down black holes in a way that's easy to understand.\n\n### What is a Black Hole?\n\nImagine a black hole like a giant vacuum cleaner in space. But instead of sucking up dirt, it pulls in everything around it, even light! That’s why we call it a \"black\" hole—because we can’t see light coming from it.\n\n### How Do They Form?\n\n1. **Stars**: Black holes start as very big stars in space. Stars are like the sun, but some are much bigger.\n  \n2. **Life Cycle of a Star**: When a giant star runs out of fuel, it can’t keep shining anymore. It collapses under its own weight.\n\n3. **Supernova**: Sometimes, when the star collapses, it explodes in a huge blast called a supernova. This is like a giant fireworks show in space!\n\n4. **Black Hole Creation**: After the explosion, if the star was big enough, it can collapse into a black hole. \n\n### Parts of a Black Hole\n\nA black hole has some important parts:\n\n- **Event Horizon**: This is like an invisible boundary around the black hole. Once something crosses this boundary, it can’t escape—not even light!\n\n- **Singularity**: This is the very center of the black hole, where all the matter gets squished together. It’s super dense and has a lot of gravity.\n\n### Gravity and Black Holes\n\nBlack holes have a lot of gravity. Gravity is what pulls things together. In fact, a black hole’s gravity is so strong that it can pull in stars and planets!\n\n### Are There Black Holes Near Us?\n\nYes, there are black holes in our universe! But don’t worry—they are really far away, and they can’t hurt us. The closest known black hole is about 1,000 light-years away!\n\n### Fun Fact!\n\n- **No Escape**: If you fell into a black hole, you wouldn’t be able to come back out. But don’t worry; black holes are far away from Earth.\n\n### Summary\n\nSo, in short, black holes are super strong vacuum cleaners in space that form when giant stars collapse. They pull in everything nearby, and they have parts like the event horizon and singularity. But they’re really far away, so we’re safe here on Earth!\n\nIf you have any questions or want to know more, just ask!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXtTzqQ3o9pQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}